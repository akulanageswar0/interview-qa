Terraform

Change Management Team
Dockerfiles


Jenkins
JenkinsFile
Sonar

Artifactory

CI/CD

Ansible
Kubernettes

ToolStack
=========
Jenkins
Docker Containerisation
Ansible
Terraform
JIRA
CI/CD
GIT
AWS

CI-->Build+Unit Test+ Archive
CD/Continuous Delivery-->Build+Unit Test+ Archive+Automated Tests+Deploy to Prod on Need Basis
CD/Continuous Deployment-->Build+Unit Test+ Archive+Automated Tests+Deploy Prod Automatic


Terraform project:
Files(main.tf,variable.tf,output.tf,backend.tf)
	Configure AWS
	Create a VPC
	Get	List the AZ
	Create a public subnet on the First available AZ
	Create an IGW for your new VPC
	Create an RouteTable for your VPC
	Associate the RouteTable to the Subnet created at AZ ap-south-1a
	Create the Security Group that's applied to Web Server EC2
	Create an EC2 inside subnet with an SG
	
	Create a Private subnet on the Second available AZ
	Create an RouteTable for your DB
	Associate the DB RouteTable to the Subnet created at AZ2 ap-south-1b
	Create the Security Group that's applied to DB Server EC2
	Create an EC2 DB Server
	

Terraform MSK project:
	AWS provider
	Role ARN
	Role Root ARN
	TF Backend S3
	AZ Data block
	CREATE/EDIT DEFAULT SECURITY GROUP
	CREATE AWS KMS KEY
	CREATE AWS CLOUDWATCH LOGS
	CREATE AWS S3 BUCKET FOR MSK
	CREATE AWS S3 BUCKET ACL FOR MSK
	CREATE AWS MSK CLUSTER
	
	
	
JenkinsFile
	Define Pipeline
	Define Agents
	Define Build Paramenters
	Define Stages
	Stage1- Clone repo and validate the code(Checkout Code)
	Stage2- PreCheck(for Stage2) & Build the Artifacts
	Stage3- CodeCoverage
	Stage4- Code Analysis
	Stage5- Quality Gates
	Stage6- Push the Build Artifact to Artifactory(Archive Artifacts)
	Stage7- Define the Docker file to build the image(Build Image)
	Stage8- Deploy to Dev Env & promote to QA Env(Deploy)
	Stage9- Run Smoke Test cases (Smoke Test)
		    Deploy to Production Env


passwordless SSH
SSH username@IP (FQDN)
ssh keygen -t rsa

Jenkins Plugins
GIT
Maven
Blueocean
GITLAB
Amazon EC2
JIRA
GreenBalls
Buildpipeline
Backup
Docker
Parameterized Trigger
Ansible

Ansible Tower only CentOS

===========================
Jenkins:
Passwordless SSH
Setup of Jenkins Master
Setup of Jenkins Slave
Jenkins Master
Plugin Management
Nodes Configuration
Tools Configuration
System Configuration
Job Configuration


=========================
Jenkins Build Trigger:
Build Triggers
 Build Periodically(similar as Cron)
 Min,Hour,DOM,Month,DOW
 PollSCM
 CI Job:
 * Poll SCM (we need to connect to a repository when there is a new checkin)
    - Central Repo
    - How frequently
 * Integrate (git pull)
 * Build (mvn package)

General
Source Code Management
Build Triggers
Build Environment
Build
Post-Build Actions

Build Triggers
 Build after other project are built
 Build Periodically
 Github hook trigger for GITSCM pollingS
 Poll SCM
 


WebHook(Gitlab) is used to trigger the build when ever there is a change in the Central Repo. It can come to jenkins and trigger the build.
===================
==============================================================
Docker file creation and the INSTRUCTIONS COMMANDS/ARGUMENTS
==============================================================
INSTRUCTIONSCOMMANDS/ARGUMENTS
------------------------------
FROM<ImageName>
RUN<OS-CMD>
CMD<Default-First-cmd>[executable, arg1, arg2]
ENTRYPOINT<Default-First-cmd>[executable, arg1, arg2, args, /bin/bash]
ENV<Variable> <Value>
ARG<Variable>=<Value>
USER<Username>
WORKDIR<path>
COPY<host-src> <container-dest>
ADD<host-src> <container-dest>
EXPOSE<port no>

=====================================
Sample dockerfile to setup Apache2
=====================================
FROM ubuntu:12.04
MAINTAINER ADAM
RUN apt-get update && apt-get install -y apache2 && apt-get clean && rm -rf /var/lib/apt/lists/*
ENV APACHE_RUN_USER www-data
ENV APACHE_RUN_GROUP www-data
ENV APACHE_LOG_DIR /var/log/apache2
EXPOSE 80
CMD ["/usr/sbin/apache2", "-D", "FOREGROUND"]


=================================
Kubernettes
Pod:A pod is the most basic unit of the Kubernetes cluster. It usually contains one or more running containers.

Service:Service object is an logical bridge between pods and end users,which provide VIP address.
ClusterIP,Nodeport
Services
 -Cluster IP
 -NodePort
 -LoadBalancer
 -Headless
 default service only on port 30000-32767


Volume:
Volumes
 -emptyDir & hostPath
 -nfs
 -awsElasticBlockStore, azureDisk, or gcePersistentDisk
 -glusterfs or cephfs
 -secret, gitRepo
 -persistentVolumeClaim
 
Namespace:Namespaces are used to organize objects in a Kubernetes cluster. They enable you to group resources together and perform actions on those resources. One of the use cases can be the creation of different environments (staging and development) for your deployed application. For example, you can create a deployment called backend-application with the staging namespace and deploy the same application in another namespace called production. While both deployments have the same name, there are no conflicts because of the difference in their namespaces.

Replicaset:The Replication Controller is used to create multiple copies of the same pod in a Kubernetes cluster. It helps ensure that at any given time, the desired number of pods specified are in the running state.

Deployment:Deployments are Kubernetes objects that are used for managing pods. The first thing a Deployment does when it’s created is to create a replicaset. The replicaset creates pods according to the number specified in the replica option.

StatefulSet:Stateful applications store the data about the state from the client requests on the server itself and use that state to process further requests. Ex: MongoDB, Cassandra, MySQL.

Stateless set:Stateless app is an application program that does not save client data generated in one session for use in the next session with that client. Ex: Amazon,Google,Facebook,Twitter, AWS S3, AWS EBS.

DaemonSet:DaemonSet is another Kubernetes object that’s used for managing pods. DaemonSets are used when you want to make sure that a particular pod runs on all nodes that belong to a cluster.

Control Plane Components:
kube-apiserver
The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.
etcd
Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.
kube-scheduler
Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.
kube-controller-manager
Control plane component that runs controller processes. Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.
cloud-controller-manager
A Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.

Node Components
kubelet
An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.
kube-proxy
kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.
Container runtime
The container runtime is the software that is responsible for running containers.

======================
--Example1:
kind: Pod                        # Object Type
apiVersion: v1                   # API version
metadata:                        # Set of data which describes the Object
  name: testpod                  # Name of the Object
spec:                            # Data which describes the state of the Object
  containers:                    # Data which describes the Container details
    - name: c00                  # Name of the Container
      image: ubuntu              # Base Image which is used to create Container
      command: ["/bin/bash", "-c", "while true; do echo Hello-Adam; sleep 5 ; done"]
      env:                       # List of environment variables to be used inside the pod
      - name: MYNAME
        value: ADAM
restartPolicy: Never# Defaults to Always

--example2:
kind: Pod
apiVersion: v1
metadata:
  name: labelspod
  labels:                         # Specifies the Label details under it
    company: wezva
    prod: kubernetes
spec:
    containers:
       - name: c00
         image: ubuntu
         command: ["/bin/bash", "-c", "while true; do echo Hello-Adam; sleep 5 ; done"]
---Example3:
kind: Pod
apiVersion: v1
metadata:
  name: nodelabels
  labels:
    env: development
spec:
    containers:
       - name: c00
         image: ubuntu
         command: ["/bin/bash", "-c", "while true; do echo Hello-Adam; sleep 5 ; done"]
    nodeSelector:                   # specifies which node to run the pod
       hardware: t2-medium

===========

https://acloudguru.com/blog/engineering/kubernetes-cheat-sheet
https://github.com/ACloudGuru-Resources/Course_EKS-Basics




jenkins build fails trouble shooting--clear

define jenkins file--clear 

how ansible will work using jenkins--Ansible Plugin

plugins required 
Ansible tower--

how complex is terraform--

have to know about docker file--clear


change management--ITIL guidelines


Shashi can i call you

=============
resume tips: 20 Apr 2020
Roles Responsibilities for CI/Continuous Deployment
* Managing Jenkins
 - Managing Security
 - Managing Tools
 - Manage Plugins
   Parameterized Trigger Pluggin
   Job configuration History plugin
   Dashboard View Plugin
   Role-based Container Strategy
   Amazon ECS Container Service
 - Jenkins CLI
 - Managing Nodes
* System Administration
 - Backup/Restore Jenkins
 - Monitor Jenkins
 
 * Setup CICD pipeline (Continuous Delivery)
   - Setup Jenkins cluster:
     * Configure Master
 * Configure Nodes
   - Setup Sonarqube
   - Setup Jfrog Artifactory
   - Setup private Docker registry
 * Monitor machines, Applications
 * Jenkins MAINTAINence
   - Backup of home directory
   - policy (purging/deleting older jobs & build output)
   - Plugins management
 * Jfrog Artifactory
   - /var/opt/jfrog/artifactory take backup
   - policy purging
     - 15 CI build
 - 15 days for Nightly builds
 - never delete release builds
 * Monitoring Builds/Jobs
   - Slave - connectivity, network, capacity
   - Master - connectivity, network, capacity
     compilation testing
     scripts
	 
	 
	 
	 Project Names need to mention
	 
	 • Automating build process for creating JAR/WAR Artifacts using Maven.
	 
	 Jenkins 
	 Declarative Templates








1.	Version control using GIT
2.	Docker
3.	Ansible Playbooks - 
4.	Declarative syntax - Yaml, Groove scripting (main)
5.	Jenkins (main) or azure pipelines
6.	Deployment to containers - Deployment strategy
7.	Use of Dockers and Containers
8.	Kubernetes exposure
9.	Working experience in AWS Cloud







Ansible Modules top:
apt or yum
ping
copy
File
Lineinfile Module

Ansible Copy/Fetch modules
file module--copies the file to the destination from source
flat--copies the file without folder structure
content_host
contents--Copies the file with the dynamic content if the file does not exist
looping--Looping is same as with items to iterate
lookup plugin--search for a particular string in the log files.
set_fact module--allows to set new variables & can be used to set host facts from a task
Lineinfile Module--ensures a particular line is in a file, or replace an existing line
start-at-task--


Ansible Playbook:
is a yaml file and will have many tasks in a defined order
to maintain the state of your machine.

Ansible Playbook:
Target Section
Variable Section
Task Section
Handler Section
Templates


--register: output ( register is used to display the output of that module in Ansible)
Asible variable hirerarchy
-runtime variable
 -playbook variable
  -Node variable
   -Group variable

Ansible project:

*JDK
*GIT
*Docker
*Maven



prometheus
grafana

Questions:
jenkins pipeline work flow
jenkins pipeline design--
groove
declarative

ci/cd complete flow
Terraform/resources
Ansible/work you have done

===============================================
Terraform Provider

Providers are API's which interact with the infrastructure components like AWS,DOcker,Kubernetes
Manages the lifecycle of resources
Providers generally are an IAAS(ex.AWS,GCP,Azure,OpenStack), PAAS(ex.Heroku), or SAAS services (eg.Terraform Cloud,DNSimple, CloudFlare)

Syntax:
provider <PROVIDER> {
 [CONFIG ...]
  ARGS....
}

Where PROVIDER is the name of a provider (eg.Docker, AWS),
CONFIG consists of one or more arguments that are specific to that provider (eg. host for Docker or Region for AWS")

Terraform Resource
Resources are the component of your infrastructure
It might be some low level component such as a physical server,virtual machine or container.Or it can be a higher level component such as an email provider, DNS record, or database provider
The resource block creates a resource of the given TYPE (first parameter) and NAME (second parameter).The combination of the type and name must be unique
Within the block is the confuration for each resource. The configuration is dependent on the type

Syntax:
provider <PROVIDER_TYPE> <NAME> {
 [CONFIG ...]
  ARGS....
}

Where PROVIDER is the name of a provider (eg. AWS)
TYPE is the type of resources to create in that provider (eg. Instance)
NAME is an identifier you can use throughout the Terraform code to refer to this resource (eg. example)
CONFIG consists of one or more arguments that are specific to that resource (eg. ami="ami-0c55b159cbfafe1f0").

Terraform Commands

Terraform init
Terraform validate
Terraform plan
Terraform apply
Terraform show
Terraform destroy
Terraform console
=================================================================
=================================================================
25 Apr21 (\Downloads\Devops\class_scnshots\AWS_Infra_demo_25Apr21)
Automate AWS Infrastructure Demo Project

Terraform Best Practices
Versioning the Terraform configuration file- git repo
Structuring the configuration files- main.tf,variables.tf,output.tf,backend.tf
Defined Modules
Distributed state- S3 versioning
Credentials- Enviornment variable,file,assume role
Set Log level for debugging
    Available log levels: TRACE,DEBUG,INFO,WARN OR ERROR
    $ export TF_LOG=TRACE
    $ export TF_LOG_PATH=terraformlog.txt

---------------------------main.tf-----------------
# provider "aws" {
# region = "ap-south-1"
  # access_key = "replace_access_key"
  # secret_key = "replace_secret_key"
# }
# #Spceify the EC2 Details
   # resource "aws_instance" "example" {
   # ami = "ami-0b99c7725b9484f9e"
   # instance_type="t2.micro"
   # tags= {
   # Name = "test Image"
   # }


# ----------------------------
# CONFIGURE OUR AWS CONNECTION
# ----------------------------
provider "aws" {
  region = "ap-south-1"
  access_key = "replace_access_key"
  secret_key = "replace_secret_key"
}

#--------------
# Create a VPC
#--------------

resource "aws_vpc" "my_vpc" {
  cidr_block       = var.vpc_cidr
  enable_dns_hostnames = true

  tags = {
    Name = "DEMO VPC - WEZVATECH"
  }
}

#-----------------------------------------------------
# GET LIST OF AVAILABILITY ZONES IN THE CURRENT REGION
#-----------------------------------------------------
data "aws_availability_zones" "all" {}


#-----------------------------------------
# Create a public subnet on the First available AZ
#-----------------------------------------
resource "aws_subnet" "public_ap_south_1a" {
  vpc_id     = aws_vpc.my_vpc.id
  cidr_block = var.subnet1_cidr
  availability_zone = data.aws_availability_zones.all.names[0]
  
  tags = {
    Name = "Public Subnet-WEZVATECH"
  }
}

#-------------------------------
# Create an IGW for your new VPC
#-------------------------------
resource "aws_internet_gateway" "my_vpc_igw" {
  vpc_id = aws_vpc.my_vpc.id

  tags = {
    Name = "DEMO IGW-WEZVATECH"
  }
}

#----------------------------------
# Create an RouteTable for your VPC
#----------------------------------
resource "aws_route_table" "my_vpc_public" {
    vpc_id = aws_vpc.my_vpc.id

    route {
        cidr_block = "0.0.0.0/0"
        gateway_id = aws_internet_gateway.my_vpc_igw.id
    }

    tags = {
        Name = "DEMO Public Route Table-WEZVATECH"
    }
}

#--------------------------------------------------------------
# Associate the RouteTable to the Subnet created at ap-south-1a
#--------------------------------------------------------------
resource "aws_route_table_association" "my_vpc_ap_south_1a_public" {
    subnet_id = aws_subnet.public_ap_south_1a.id
    route_table_id = aws_route_table.my_vpc_public.id
}
#------------------------------------------------------------
# Create the Security Group that's applied to Web Server EC2
#------------------------------------------------------------
resource "aws_security_group" "instance" {
  name = "my_instance"
  vpc_id = aws_vpc.my_vpc.id
  
  # Allow all outbound 
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
  # Inbound SSH access from the VPC
  ingress {
      from_port   = 22
      to_port     = 22
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
  }
  # Inbound for Web Server
  ingress {
      from_port   = 8080
      to_port     = 8080
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
     Name = "SG Web Server EC2"
  }
}
#----------------------------------
# Create an EC2 inside subnet with an SG
#----------------------------------
resource "aws_instance" "server" {
  ami           = var.amiid
  instance_type = var.type
  key_name      = var.pemfile
  vpc_security_group_ids = [aws_security_group.instance.id]
  subnet_id = aws_subnet.public_ap_south_1a.id
  availability_zone = data.aws_availability_zones.all.names[0]
  associate_public_ip_address = true

  user_data = <<-EOF
              #!/bin/bash
              echo '<html><body><h1 style="font-size:50px;color:blue;">WEZVA TECHNOLOGIES (ADAM) <br> <font style="color:red;"> www.wezva.com <br> <font style="color:green;"> +91-9876543210 </h1> </body></html>' > index.html
              nohup busybox httpd -f -p 8080 &
              EOF
  tags = {
     Name = "Web Server-WEZVATECH"
  }

#-----------------------------------------
# Create a Private subnet on the Second available AZ
#-----------------------------------------
resource "aws_subnet" "private_ap_south_1b" {
  vpc_id     = aws_vpc.my_vpc.id
  cidr_block = var.subnet2_cidr
  availability_zone = data.aws_availability_zones.all.names[1]
  
  tags = {
    Name = "Private Subnet-WEZVATECH"
  }
}

#----------------------------------
# Create an RouteTable for your DB
#----------------------------------
resource "aws_route_table" "my_vpc_private" {
    vpc_id = aws_vpc.my_vpc.id

    route {
        cidr_block = "0.0.0.0/0"
        instance_id = aws_instance.server.id
    }

    tags = {
        Name = "DEMO Private Route Table-WEZVATECH"
    }
}
#--------------------------------------------------------------
# Associate the DB RouteTable to the Subnet created at ap-south-1b
#--------------------------------------------------------------
resource "aws_route_table_association" "my_vpc_ap_south_1b_private" {
    subnet_id = aws_subnet.private_ap_south_1b.id
    route_table_id = aws_route_table.my_vpc_private.id
}
#------------------------------------------------------------
# Create the Security Group that's applied to DB Server EC2
#------------------------------------------------------------
resource "aws_security_group" "db" {
  name = "adam-example-db"
  vpc_id = aws_vpc.my_vpc.id
  
  # Allow all outbound 
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
  # Inbound SSH access
  ingress {
      from_port   = 3306
      to_port     = 3306
      protocol    = "tcp"
      security_groups = [aws_security_group.instance.id]
      cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
     Name = "SG DB Server EC2"
  }
}
#------------------------
# Create an EC2 DB Server
#------------------------
resource "aws_instance" "db" {
  ami           = var.amiid
  instance_type = var.type
  key_name      = var.pemfile
  vpc_security_group_ids = [aws_security_group.db.id]
  subnet_id = aws_subnet.private_ap_south_1b.id
  availability_zone = data.aws_availability_zones.all.names[1]
  associate_public_ip_address = true

  tags = {
     Name = "DB Server-WEZVATECH"
  }

----------------------------------------------------

----------------------variables.tf----------------------
variable "vpc_cidr" {
  default     = "10.0.0.0/16"
}

variable "subnet1_cidr" {
  default     = "10.0.0.0/24"
}

variable "subnet2_cidr" {
  default     = "10.0.1.0/24"
}

variable "amiid" {
  default     = "ami-0d758c1134823146a"
}

variable "type" {
  default     = "t2.micro"
}

variable "pemfile" {
  default     = "master"
}
    
----------------------------output.tf-----------
output "vpc_id" {
  value = "${aws_vpc.my_vpc.id}"
}
output "public_subnets" {
  value = ["${aws_subnet.public_ap_south_1a.id}"]
}
output "public_route_table_ids" {
  value = ["${aws_route_table.my_vpc_public.id}"]
}
output "public_instance_ip" {
  value = ["${aws_instance.server.public_ip}"]
}
output "private_subnets" {
  value = ["${aws_subnet.private_ap_south_1b.id}"]
}
output "private_route_table_ids" {
  value = ["${aws_route_table.my_vpc_private.id}"]
}
output "private_instance_ip" {
  value = ["${aws_instance.db.private_ip}"]
}
-------------------------------------------------------------------


============================================
What is the difference between Virtualization and Containerization? 
Resource optimization is the main difference, Guest OS can block all the resources allocated it but containers dont block the resources. Apart from this provisioning/boot up time is very less in containers as they are very less size compared to VM. Scalability, high availability are achieved easily. Containers are immutable in nature they work same across all environment. basically build once and run anywhere. Performance is improved as containers can communicate with physical hardware directly where as in virtualization there is a hypervisor layer.

How to do the port mapping to the container?
We will docker -p [host-port]:[container-port] option

How can you get the shell access of running container?
We need to issue docker exec -it [container-ID] bash command to get inside of the container.

What is the difference between RUN and CMD?
RUN is the instruction executes at the time of image creation, CMD is the instruction executes at the time of Container creation. CMD commands keeps the container running. RUN is used to install packages, configure something at the time of image creation

What is Dockerfile?
Dockerfile is the declarative way of creating our own images. We need to use Dockerfile instructions to create the image.

How do you build Docker image?
We need to use docker build -t [REPO-URL]/[USERNAME]/[IMAGE-NAME]:version . 
We should have Dockerfile available in the directory where we run docker build command

What is the difference between ADD vs COPY?
ADD and COPY does the same, copying the files from local to container. But ADD has some extra capabilities.
1. ADD can download the resources from internet to Container.
2. ADD can directly direct unzip the file into container if it is recognized format.

What is the Difference between CMD and ENTRYPOINT?
CMD and ENTRYPOINT both are useful to run the containers, but there few differences.
1. CMD can be overridden, ENTRYPOINT cant be overriden
2. If you try to override ENTRYPOINT it will go and append.
3. We can mix CMD and ENTRYPOINT for best results.
4. We can provide default arguments through CMD to ENTRYPOINT.
5. Users can override those default arguments at runtime from command line.

What is the Difference between ARG and ENV?
1. ARG is the instruction used to hold variables at the build time. We can pass the values from command line using --build-arg KEY=VALUE 
docker build --build-arg USERNAME=sivakumar -t image-name:version . 

2. ARG variables are valid only for build time not valid at the time container running. 

3. ARG is the only instruction that can be written before FROM, ARG before FROM is valid till FROM statement only. 

4. ENV is the instruction used to hold variables at the build time and container as well. 

5. We mix ENV and ARG for best results, like assign the value of ARG to ENV variable.

What are Docker volumes?
Docker by default remove the data when it is deleted. So when we are running stateful applications like databases we need to use Docker volumes that will not delete the data after container is deleted.
to create volume
Named volumes
-------------------
docker volume create [name-of-volume]
docker volume ls
docker run -v [name-of-volume]:[container-path] [image]:[version]

there is another way anonymous volumes, i.e without creating volumes we can directly mount the directory in host to the container. But this is not recommended.

docker run -v /some/path/inhost:/container/path [image]:[version]

Tell us the best practices you implemented while creating docker images?

1. Use light weight base images like alpine, busybox, etc.
2. Multi stage builds, remove unnecessary installations.
3. Non root users
4. Use volumes for stateful applications
5. Use docker compose
6. Use env variables instead of hard coding.
7. Use dedicated custom networks.
8. Dont keep secrets in images.
9. Scan the images and fix vulnerabilities.
10. Limit resources CPU, RAM
11. Configure health checks
